{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is an introduction to me and my writings. </p>"},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p> <p>Example: link to Mermaid Diagrams under <code>Features</code></p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/","title":"1. Introduction to Generative AI","text":""},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#goals-of-a-generative-model","title":"Goals of a generative model","text":"<p>The primary objective of a generative model (with parameters \\(\\theta\\)) is to approximate via \\(P_\\theta(\\mathbf{x})\\) the data distribution \\(P_{data}(\\mathbf{x})\\) given a sufficiently large set of training samples from an independent and identically distributed training distribution \\(P_{train}(\\mathbf{x})\\). </p> <p>However an effective generative model would also be able to capture patterns, dependencies and features present in the data. And there is a trade-off between maximising likelihood (overfitting to \\(P_{train}(\\mathbf{x})\\)) vs extracting the latent space structure so that features can be learnt and modelled.</p> <p>Along with these primary objectives, a good generative model should also support efficient sampling from \\(P_\\theta(\\mathbf{x})\\) laying emphasis on diversity and fidelity of generated samples.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#model-parameter-estimation","title":"Model parameter estimation","text":"<p>Most machine learning techniques have to make a choice on how they estimate parameters of the model, generally based on the phenomenon they try to model. </p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#maximum-a-posteriori-map","title":"Maximum a posteriori (MAP)","text":"<p>If there exists some kind of a-priori knowledge of the phenomenon (e.g. physical laws or financial laws), then these could be used as a start to the modelling process. Further fine-tuning of the parameters are then made based on the training samples. MAP incorporates prior knowledge through a prior distribution \\(P(\\theta)\\). It finds the parameter that maximises the posterior distribution \\(P(\\theta | \\mathbf{x})\\). Using Bayes theorem, the posterior distribution is given by</p> \\[ P(\\theta | \\mathbf{x}) = \\frac{P(\\mathbf{x} | \\theta) P(\\theta)}{P(\\mathbf{x})} \\] <p>Since \\(P(\\mathbf{x})\\) does not depend on \\(\\theta\\), it can be ignored during optimisation. Thus the MAP estimate \\(\\hat{\\theta}_{\\text{MAP}}\\)\u200b is the parameter that maximises the posterior (log for simplifying the computations)</p> \\[ \\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} \\big( \\log P(\\mathbf{x} | \\theta) + \\log P(\\theta) \\big) \\]"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#maximum-likelihood-estimation-mle","title":"Maximum  likelihood estimation (MLE)","text":"<p>In generative models typically, there is no prior distribution available to begin the parameter optimisation. Therefore frequentist methods of estimation like maximum likelihood are employed. The objective here reduces to finding the parameters \\(\\hat{\\theta}_{\\text{MLE}}\\) that maximises the likelihood of the observed data \\(\\mathbf{x}\\). </p> \\[ \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta} \\big(\\log P(\\mathbf{x} | \\theta) \\big) \\] Frequentist statistics <p>Frequentist statistics is a framework for statistical inference based on the idea that probabilities are long-run frequencies of events. Frequentist  methods do not incorporate prior information about the parameters. Instead, they rely solely on the observed data to make inferences.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#analysing-probability-distributions","title":"Analysing probability distributions","text":"<p>Since we aim to model probability distributions, we need some means of analysing different distributions as well as metrics for comparing distributions. Shannon's information theory gives us a good basis for analysing probability distributions in terms of their information content and compressibility.</p> Shannon's Information Theory <p>In A Mathematical Theory of Communication, Shannon laid the groundwork for  information theory by introducing concepts like entropy, channel capacity, and coding theory. His work showed how information can be transmitted reliably over noisy channels and how data can be compressed efficiently.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#entropy-of-a-distribution","title":"Entropy of a distribution","text":"<p>The idea of entropy was introduced to understand the information content of a distribution. Information theory defines information in terms of the uncertainty or surprise associated with random variable. Entropy quantifies the expected amount of \"surprise\" in observing outcomes from a distribution. For a random variable \\(\\mathbf{x}\\) with possible outcomes \\(x\\) the entropy is given by</p> \\[ H(\\mathbf{x}) = -\\sum_x P(x) \\log P(x) \\] <p>A high entropy indicates a distribution with high uncertainty (e.g., a uniform distribution where all outcomes are equally likely), while a low entropy indicates a more predictable distribution (e.g., a delta distribution concentrated on a single outcome). The Shannon entropy, when calculated with a base-2 logarithm, measures information in bits, providing a direct interpretation of information content that is both intuitive and mathematically optimal for digital communication. It effectively gives us the minimum bits needed to optimally compress a random variable given its probability distribution.</p> <p>In generative AI, the concept of entropy when extended to cross-entropy between two probabilities and its associated distance metrics gives us the necessary tools to compare probability distributions.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#cross-entropy-of-two-distributions","title":"Cross entropy of two distributions","text":"<p>Cross-entropy is a measure of the difference between two probability distributions and is widely used as a loss function in machine learning (ML) models. When used for example in generative AI, cross-entropy quantifies the \"distance\" between the true distribution (the actual samples from \\(P_{data}\\)) and the model distribution (generated from \\(P_\\theta\\)). The lower the cross-entropy, the closer the model's generation mimics the data distribution. </p> <p>For discrete probability distributions \\(P\\) and \\(Q\\) over the same set of events, the cross-entropy \\(H(P,Q)\\) is given by </p> \\[ H(P, Q) = -\\sum_{x}P(x)\\log Q(x) \\] <p>Cross-entropy can be decomposed into two parts: the entropy of the true distribution and an additional term known as the KL divergence (or relative entropy) between the true distribution and the model distribution. This decomposition highlights how cross-entropy combines information from the true distribution\u2019s inherent uncertainty and the \"extra cost\" of using the model distribution </p> \\[ H(P, Q) = H(P) + D_{KL}(P||Q) \\] <p>Intuitively, in Information Theory parlance, the cross entropy decomposition can be thought of as representing the number of bits needed to compress the true distribution optimally and the KL divergence being the additional bits needed if information is compressed according to the model distribution.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#distance-measure-kl-divergence","title":"Distance measure (KL divergence)","text":"<p>This leads us to a possible distance measure between distributions \\(P\\) and \\(Q\\) when using Shannon's entropy and cross-entropy which is the Kullback-Leibler (KL) divergence given by</p> \\[ D_{KL}\u200b(P||Q)=\\sum_x\u200bP(x)log\\frac{P(x)}{Q(x)}\u200b \\] <p>The divergence \\(D_{KL}(P||Q) \\ge 0\\) for all \\(P\\), \\(Q\\) with equality if and only if \\(P = Q\\) and therefore it can be used as a reasonable measure to compare distributions although it is not a true metric.</p> Metric space theory and true distance metrics <p>The notion of a metric comes from metric space theory in mathematics, particularly within topology and geometry. The metric concept provides a formal way to measure \"distance\" between elements in a space. A metric on  a set \\(X\\) is a function \\(d: X \\times X \\rightarrow \\mathbb{R}\\) that  satisfies the following properties:</p> <ol> <li>Non-negativity: \\(d(x, y) \\geq 0\\) </li> <li>Identity of indiscernibles: \\(d(x, y) = 0\\) if and only if \\(x=y\\)</li> <li>Symmetry: \\(d(x,y)=d(y,x)\\) </li> <li>Triangle inequality: \\(d(x, z) \\leq d(x, y) + d(y, z)\\)</li> </ol> <p>The KL divergence is not a true distance metric because it is not symmetric and it does not obey the triangle inequality. However there are other measures of distance (Wasserstein, Jensen-Shannon etc.) based on other \"divergences\" and \"entropy\" definitions (generalised as f-divergences), some of which are true metrics.</p>"},{"location":"Generative%20AI/1.%20Introduction%20to%20Generative%20AI/#kl-divergence-in-generative-modelling","title":"KL divergence in generative modelling","text":"<p>For a generative model \\(P_\\theta(\\mathbf{x})\\) with the corresponding data distribution \\(P_{data}(\\mathbf{x})\\), the KL divergence can be stated as</p> \\[ \\begin{split} D_{KL}\u200b(P_{data}||P_\\theta) &amp;= \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log \\left( \\frac{P_{data}(\\mathbf{x})}{P_\\theta(\\mathbf{x})} \\right) \\right] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log P_{data}(\\mathbf{x}) \\right] - \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log {P_\\theta(\\mathbf{x})} \\right] \\end{split}\u200b \\] <p>And since the first terms does not depend on \\(P_\\theta\\), minimising KL divergence in generative modelling reduces to maximising the expected log-likelihood (MLE)</p> \\[ \\arg \\min_{\\theta} D_{KL}\u200b(P_{data}||P_\\theta) = \\arg \\max_{\\theta}\\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log {P_\\theta(\\mathbf{x})} \\right] \\] <p>This also shows us that although we can compare models \\(P_{\\theta_1}\\) and \\(P_{\\theta_2}\\) in which of them more closely models the true data distribution \\(P_{data}\\), we cannot know how close we are to the \\(P_{data}\\) itself with either of them.</p> <p>In practise we approximate the expected log-likelihood with the empirical log-likelihood over training samples \\(D\\). This follows from Monte Carlo estimation.</p> \\[ \\hat{\\theta}_{\\text{MLE}} = \\max_{\\theta} \\frac {1}{|D|}\\sum_{x \\in D}\\log P_\\theta(x) \\] Monte Carlo estimation <p>Monte Carlo Estimation is a method for approximating an unknown quantity, typically an expectation, integral, or sum, using random sampling. It is  particularly useful when direct analytical computation is intractable due to  high dimensionality or complexity.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/","title":"2. Autoregressive models","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#introduction","title":"Introduction","text":"<p>Autoregressive models as the name implies, generate data by predicting each element sequentially based on the elements previously generated. They are naturally aligned to tasks involving sequential dependence like natural language, audio and time-series data. However, autoregressive models have also been successfully applied to image generation.</p> <ul> <li>GPT (Generative Pre-trained Transformer) is a language model that predicts the next word or token based on previously generated text.</li> <li>PixelCNN and PixelRNN generate images pixel by pixel, with each pixel depending on the pixels generated before it.</li> <li>WaveNet produces audio samples one at a time, with each sample conditioned on prior samples, making it suitable for realistic speech and audio synthesis.</li> </ul>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#factorisation-of-a-probability-distribution","title":"Factorisation of a probability distribution","text":"<p>In the autoregressive approach we are either dealing with an ordered sequence (language) or unordered vector samples (images) and in general the data can be thought of as multi-dimensional vectors and their probability distributions are best represented as joint distributions over all these dimensions. A joint distribution over an n-dimensional vector can be stated accurately using the chain rule of probabilities as</p> \\[ p(x_1, x_2, \\dots, x_n) = p(x_n | x_{n-1}, \\dots, x_2, x_1) \\dots p(x_3 | x_2, x_1) \\cdot p(x_2 | x_1) \\cdot p(x_1) \\] <p>The principle behind autoregression is to compute this factorisation sequentially. For large dimensions this factorisation requires an exponentially large number of parameters (number of possible states that the distribution can take, also known as the support of the distribution). A Bayesian network is a graphical representation of such a probability distribution and in the unsimplified case above, a fully connected directed acyclic graph (DAG) with nodes representing the elements of the vector and the directed edges representing the dependence.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#conditional-independence-in-a-bayesian-network","title":"Conditional independence in a Bayesian network","text":"<p>One way to simplify the computation of the joint distribution is to assume conditional independence - equivalent to removing certain edges from the Bayesian network. The simplest example of this would be a Markov model where an element (or node in a DAG) is independent of every other (historical) element given the immediate previous one</p> \\[ p(x_{i} \\perp x_{i-2},\\dots,x_2,x_1 | x_{i-1}) \\] <p>and the chain rule factorisation simplifies to</p> \\[ p(x_1, x_2, \\dots, x_n) = p(x_n | x_{n-1}) \\dots p(x_3 | x_2) \\cdot p(x_2 | x_1) \\cdot p(x_1) \\] <p>A general Bayesian network would be one where every value is conditionally dependent on a few others (\\(\\ll n\\)) thereby simplifying the joint distribution to</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) denotes the subset of elements of \\(\\mathbf{x}\\) on which \\(x_i\\) is conditionally dependent.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#chain-rule-based-autoregressive-generators","title":"Chain rule based autoregressive generators","text":"<p>Another way to simplify the computation is to assume that there exists a computable function that can closely approximate the conditional probabilities. If we are dealing with discrete variables, then this function is a probability mass function (PMF), while if we are dealing with continuous variables then it is the probability density function (PDF). The structure remains the same and for the case of a fully connected network (chain rule factorisation) the implementation can be represented by the figure below where the \\(N_i\\) blocks represent the individual functional approximations of the conditional probabilities with each model (increasing \\(i\\)) being progressively more complex than the previous one.</p> <pre><code>graph TD\n    %% Input Nodes\n    x0((x.)) --&gt; N1\n    x1((x\u2081)) --&gt; N2\n    x1((x\u2081)) --&gt; N3\n    x1((x\u2081)) --&gt; N4\n    x2((x\u2082)) --&gt; N3\n    x2((x\u2082)) --&gt; N4\n    x3((x\u2083)) --&gt; N4\n\n    %% Process Blocks\n    N1[N\u2081] --&gt; y1((\"p(x\u2081)\"))\n    N2[N\u2082] --&gt; y2((\"p(x\u2082|x\u2081)\"))\n    N3[N\u2083] --&gt; y3((\"p(x\u2083|x\u2082,x\u2081)\"))\n    N4[N\u2084] --&gt; y4((\"p(x\u2084|x\u2083,x\u2082,x\u2081)\"))\n\n    %% Additional Inputs\n    x4((x\u2084))</code></pre> <p>Fully visible sigmoid belief networks (FVSBN) use logistic regression for each of the individual models. Results however are not good with this technique for image generation, since logistic regression is not complex enough to capture relationships in the images.</p> <p>Using neural networks instead, works better as is the case with neural autoregressive distribution estimator (NADE) which uses single layer neural networks for each of the models. Unlike logistic regression, a neural network can introduce non-linearities and is therefore much more flexible in learning features and the results are much more impressive. Parameter sharing (tying weights) reduces parameters, speeds up training and generation. If we use a similar architecture to model a general Bayesian network with conditional independence simplifications it reduces to</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p_{neural}(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) are the nodes of the DAG on which \\(x_i\\) is conditionally dependent. Additionally this Bayesian network needs to satisfy the \"Markov\" ordering constraints imposed by the chain rule factorisation (so that generation is possible), which means</p> \\[ \\hat{\\mathbf{x}_i} \\subset \\{x_1, x_2, \\dots x_j\\} \\implies j &lt; i \\] Universal Approximation Theorem <p>A feedforward neural network with at least one hidden layer and a sufficiently large number of neurons can approximate any continuous function to any desired degree of accuracy, provided the network uses a non-linear activation function (like a sigmoid or ReLU)</p> <p>Generative models using the chain rule factorisation however require as many neural networks as there are input dimensions - each modelling a single conditional probability.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#generating-from-the-autoregressive-model","title":"Generating from the autoregressive model","text":"<p>Generating from the chain rule based autoregressive model has a similar architecture to the one used for training.</p> <ul> <li>\\(x_1\\) is sampled from the marginal prior distribution \\(p(x_1)\\).</li> <li>This is input to the first neural network \\(N_2\\) to obtain \\(p(x_2|x_1)\\) and \\(x_2\\) is sampled from this distribution.</li> <li>This process is repeated sequentially for all \\(x_{i \\le n}\\) to obtain the complete generation.</li> </ul> <p>As is obvious from the steps above, the generative process from an autoregressive model is sequential with each \\(x_i\\) from the generated vector, generated sequentially from the previously generated values.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#autoencoders-as-autoregressive-generators","title":"Autoencoders as autoregressive generators","text":"<p>Autoencoders are generally used to obtain a compressed representation of the inputs. Their structure being that of a generalised Bayesian network can be used to modify them to function as an autoregressive model under certain constraints. The conditional probabilities are</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p_{neural}(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) are the nodes of the DAG on which \\(x_i\\) is conditionally dependent.</p> <pre><code>graph TD\n    %% Input Nodes\n    x1((x\u2081)) --&gt; N\n    x2((x\u2082)) --&gt; N\n    x3((x\u2083)) --&gt; N\n    x4((x\u2084)) --&gt; N\n\n    %% Process Blocks\n    N[Autoencoder] --&gt; y4((\"p(x\u1d62|x\u0302\u1d62)\"))</code></pre> <p>A vanilla autoencoder however, is not a generative model. Since the DAG modelled by the neural network does not have any inherent ordering, it does not provide us with a meaningful way to generate samples from the model because the model is non-causal with respect to the input sequence.</p> <p>If we are able to constrain the DAG so as to force some kind of sequential ordering on the dependencies similar to what we achieve using chain rule factorisation, we can use the autoencoder as an autoregressive generative model. This is precisely what is done in a masked autoencoder for distributed estimation (MADE) where masks are used to disallow certain paths in the DAG so as to follow an ordered dependency sequence. The advantage of such a model over the chain rule factorisation is that a single neural network (deep) can model the joint probability distribution.</p> <p>The generative process is however exactly the same as before and is sequential and time consuming.</p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#rnns-in-autoregressive-models","title":"RNNs in autoregressive models","text":"<p>The AR formulation above in terms of a sequential conditional dependence Bayesian network looks like something that could be modelled using Recurrent Neural Networks (RNNs) which have traditionally been used for modelling time series or sequences with causal restrictions. The obvious challenge when one looks at the chain rule factorisation is that as the sequence progressed, \\(\\hat{\\mathbf{x}_i}\\) (nodes of the DAG on which \\(x_i\\) is conditionally dependent) becomes larger and therefore modelling the relationships become complex. Similar to autoencoders which model the data distribution as a compressed representation, the RNNs solve this problem by modelling the conditional dependence using a compressed hidden state given by</p> \\[ h_i\u200b=\\sigma(W_{hh}\u200bh_{i\u22121}\u200b+W_{xh}\u200bx_i+b_h\u200b) \\] <p>where \\(\\sigma\\) is an activation function, \\(W_{hh}\u200b\\) and \\(W_{xh}\\) are weight matrices and \\(b_h\\) is a bias vector. \u200bThe output function given by</p> \\[ y_i\u200b=\\phi(W_{hy\u200b}h_i\u200b+b_y\u200b) \\] <p>models \\(x_i | \\hat{\\mathbf{x}_i}\\). </p>"},{"location":"Generative%20AI/2.%20Autoregressive%20models/#rnns-applied-to-language-modelling","title":"RNNs applied to language modelling","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#rnns-applied-to-image-generation-pixelrnn","title":"RNNs applied to image generation - PixelRNN","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#convolutions-for-improvement-pixelcnn","title":"Convolutions for improvement - PixelCNN","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#attention-and-transformers","title":"Attention and Transformers","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#attention-mechanism","title":"Attention mechanism","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#transformers","title":"Transformers","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#language-modelling-and-llms","title":"Language modelling and LLMs","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#autoregressive-models-in-audio","title":"Autoregressive models in audio","text":""},{"location":"Generative%20AI/2.%20Autoregressive%20models/#summary-of-autoregressive-models","title":"Summary of autoregressive models","text":"<p>Pros, Cons, Limitations, Areas where they are advantageous, technology, technological constraints </p>"},{"location":"Generative%20AI/Variational%20auto-encoders%20%28VAE%29/","title":"Variational auto encoders (VAE)","text":""},{"location":"Generative%20AI/Variational%20auto-encoders%20%28VAE%29/#motivation-for-latent-variable-models","title":"Motivation for latent variable models","text":"<ol> <li>Capture latent features in a complex dataset</li> <li>Latent variables \\(z\\) correspond to high level features</li> </ol>"},{"location":"Generative%20AI/Variational%20auto-encoders%20%28VAE%29/#motivation-for-latent-variable-models_1","title":"Motivation for latent variable models","text":"<p>Capture latent features in a complex dataset Latent variables \\(z\\) correspond to high level features \\(p(x | z)\\) is easier to compute than \\(p(x)\\) and we could identify features using \\(p(z|x)\\) Use NN to model </p> Phasellus posuere in sem ut cursus <ul> <li>Unsupervised representation learning</li> <li>Shallow LV model - Mixture of gaussians</li> </ul> <p>where the feature \\(\\mathbf{z}\\) is simply a categorical variable </p> \\[ \\mathbf{z} \\sim categorical(1, ...., K) \\] <p>while \\(\\mathbf{x}\\) is a mixture of gaussians</p> \\[ p(\\mathbf{x | z} = k) = \\mathcal{N}(\\mu_k, \\Sigma_k) \\] <p>Can use unsupervised clustering to extract latent features \\(z\\) Can combine simple models to form more complex models</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/11/20/cnn/","title":"Convolutional neural network (CNN)","text":"<p>Blog excerpt here</p> <p>Rest of blog here with details ...</p>","tags":["ai","neural-net","convolution"]},{"location":"blog/2024/11/30/gnn/","title":"Graph neural network (GNN)","text":"<p>Blog excerpt here</p> <p>Rest of blog here with details ...</p>","tags":["ai","neural-net","graph"]},{"location":"blog/2024/11/25/rnn/","title":"Recurrent neural network (RNN)","text":"<p>Blog excerpt here</p> <p>Rest of blog here with details ...</p>","tags":["ai","neural-net"]},{"location":"blog/2024/11/25/self-supervised/","title":"Self supervised learning","text":"<p>Blog excerpt here</p> <p>Rest of blog here with details ...</p>","tags":["ai","neural-net"]},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/neural-networks/","title":"neural networks","text":""},{"location":"blog/category/learning/","title":"learning","text":""}]}