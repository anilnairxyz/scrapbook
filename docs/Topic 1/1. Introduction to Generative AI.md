## Goals of a generative model ##

The primary objective of a generative model (with parameters $\theta$) is to approximate via $p_\theta(x)$ the data distribution $p_{data}(x)$. The straightforward way of doing this would be using maximum likelihood estimation (MLE). 

However an effective generative model would also be able to capture patterns, dependencies and features present in the data. And there is a trade-off between maximising likelihood (overfitting) vs extracting the latent space structure so that features can be learnt and modelled.

Along with these primary objectives, a good generative model should also support efficient sampling from $p_\theta(x)$ laying emphasis on diversity and fidelity of generated samples.

## Modelling a probability distribution ##

When training a generative model, what are we actually modelling? One way to look at it, is to say that we are modelling a probability distribution $p(x)$ for a data sample $x$. And if $p(x)$ models the probability distribution of the data accurately, any samples generated by that model will therefore closely represent a real data sample. To achieve this we also need some measure of similarity that can be efficiently computed for comparing model probability distributions with the data distribution.

In general, since the samples (images / text) that we aim to generate are multi-dimensional vectors, the probability distributions are best represented as joint distributions over all these dimensions. A joint distribution over an n-dimensional vector can be stated accurately using the chain rule of probabilities as 

$$
p(x_1, x_2, \dots, x_n) = p(x_n | x_{n-1}, \dots, x_2, x_1) \dots p(x_3 | x_2, x_1) \cdot p(x_2 | x_1) \cdot p(x_1)
$$

For large dimensions this factorisation requires an exponentially large number of parameters (number of possible states that the distribution can take, also known as the support of the distribution). A Bayesian network is a graphical representation of such a probability distribution and in the unsimplified case above, a fully connected directed acyclic graph (DAG).

### Conditional independence in a Bayesian network ###

One way to simplify the computation of the joint distribution is to assume conditional independence - equivalent to removing certain edges of a Bayesian network. The simplest example of this would be a Markov model where a value (or node in a DAG) is independent of every other (historical) value given the immediate previous one

$$
p(x_{i} \perp x_{i-2},\dots,x_2,x_1 | x_{i-1})
$$

and the chain rule factorisation simplifies to 

$$
p(x_1, x_2, \dots, x_n) = p(x_n | x_{n-1}) \dots p(x_3 | x_2) \cdot p(x_2 | x_1) \cdot p(x_1)
$$

A more general Bayesian network would be one where every value is conditionally dependent on a few others ($\ll n$) thereby simplifying the joint distribution to 

$$
p(x_1, x_2, \dots, x_n) = \prod_{i=1}^n p(x_i | \hat{\mathbf{x}_i})
$$

where $\hat{\mathbf{x}_i}$ denotes the subset of elements of $\mathbf{x}$ on which $x_i$ is conditionally dependent. 

### Modelling conditional probabilities using a neural network ###

Another way to simplify the computation is to assume that there exists a computable function that can closely approximate the conditional probabilities. Using neural networks would be a practical way of modelling these individual functional approximations in a Bayesian network.

$$
p(x_1, x_2, \dots, x_n) = \prod_{i=1}^n p_{neural}(x_i | \hat{\mathbf{x}_i})
$$

 A sufficiently deep neural network can approximate any function.
 
 ???+ note "Universal Approximation Theorem"

	A feedforward neural network with at least one hidden layer and a 
	sufficiently large number of neurons can approximate any continuous function
	to any desired degree of accuracy, provided the network uses a non-linear
	activation function (like a sigmoid or ReLU)

???+ info "Phasellus posuere in sem ut cursus"

    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod
    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor
    massa, nec semper lorem quam in massa.

If we are dealing with discrete variables, then this function is a probability mass function (PMF), while if we are dealing with continuous variables then it is the probability density function (PDF).

Some families of generative models like autoregressive models and variational autoencoders use this approximation to model the data distribution.