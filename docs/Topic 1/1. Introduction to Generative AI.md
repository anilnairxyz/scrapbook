## Goals of a generative model ##

The primary objective of a generative model (with parameters $\theta$) is to approximate via $p_\theta(\mathbf{x})$ the data distribution $p_{data}(\mathbf{x})$ given a sufficiently large set of training samples from an independent and identically distributed training distribution $p_{train}(\mathbf{x})$. 

However an effective generative model would also be able to capture patterns, dependencies and features present in the data. And there is a trade-off between maximising likelihood (overfitting to $p_{train}(\mathbf{x})$) vs extracting the latent space structure so that features can be learnt and modelled.

Along with these primary objectives, a good generative model should also support efficient sampling from $p_\theta(\mathbf{x})$ laying emphasis on diversity and fidelity of generated samples.

## Analysing probability distributions ##

Since we aim to model probability distributions, we need some means of analysing different distributions as well as metrics for comparing distributions. Shannon's information theory gives us a good basis for analysing probability distributions in terms of their information content and compressibility.

???+ info "Shannon's Information Theory"

    In [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), Shannon laid the groundwork for 
    information theory by introducing concepts like entropy, channel capacity,
    and coding theory. His work showed how information can be transmitted
    reliably over noisy channels and how data can be compressed efficiently.

### Entropy of a distribution ###

The idea of entropy was introduced to understand the information content of a distribution. Information theory defines information in terms of the uncertainty or surprise associated with random variable. Entropy quantifies the expected amount of "surprise" in observing outcomes from a distribution. For a random variable $X$ with possible outcomes $x$ the entropy is given by

$$
H(X) = -\sum P(x) \log_2P(x)
$$

A high entropy indicates a distribution with high uncertainty (e.g., a uniform distribution where all outcomes are equally likely) while a low entropy indicates a more predictable distribution (e.g., a delta distribution concentrated on a single outcome). Although there could be other measures of entropy based on other comparison methods, the Shannon entropy, when calculated with a base-2 logarithm, measures information in bits, providing a direct interpretation of information content that is both intuitive and mathematically optimal for digital communication. In combination with its associated distance measure (KL divergence) it also has its use in generative AI.
### Cross entropy of two distributions ###

Cross-entropy is a measure of the difference between two probability distributions and is widely used as a loss function in machine learning (ML) models, particularly for classification tasks. When used for example in discriminative learning tasks like classification, cross-entropy quantifies the "distance" between the true distribution (the actual labels of the data) and the predicted distribution (the probabilities predicted by the model). The lower the cross-entropy, the closer the model's predictions are to the actual labels. 

For discrete probability distributions $P$ and $Q$ over the same set of events, the cross-entropy $H(P,Q)$ is given by 

$$
H(P, Q) = -\sum_{x}P(x)log_2Q(x)
$$

Cross-entropy can be decomposed into two parts: the entropy of the true distribution and an additional term known as the KL divergence (or relative entropy) between the true distribution and the predicted distribution. This decomposition highlights how cross-entropy combines information from the true distribution’s inherent uncertainty and the "extra cost" of using the predicted distribution 

$$
H(P, Q) = H(P) + D_{KL}(P||Q)
$$

- Kullback -Leibler (KL) Divergence 

### Distance measures ###

This leads us to a possible distance measure between distributions $P$ and $Q$ when using Shannon's entropy and cross-entropy which is the Kullback-Leibler (KL) divergence given by

$$
D_{KL}​(P||Q)=\sum_x​P(x)log\frac{P(x)}{Q(x)}​
$$

???+ info "Metric space theory and true distance metrics"

    The notion of a metric comes from metric space theory in mathematics,
    particularly within topology and geometry. The metric concept provides a
    formal way to measure "distance" between elements in a space. A **metric** on
     a set $X$ is a function $d: X \times X \rightarrow \mathbb{R}$ that
     satisfies the following properties
		- Non-negativity: $d(x, y) \geq 0$
		- Identity of indiscernibles: $d(x, y) = 0$ if and only if $x=y$
		- Symmetry: $d(x,y)=d(y,x)$
		- Triangle inequality: $d(x, z) \leq d(x, y) + d(y, z)$

The KL divergence is not a true metric because it is not symmetric and it does not obey the triangle inequality. However there are other measures of distance (Wasserstein, Jensen-Shannon etc.) based on other "divergences" and "entropy" definitions (generalised as f-divergences), some of which are true metrics.
